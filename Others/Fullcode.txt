# datasets/broker_crowd_dataset.py

# datasets/broker_crowd_dataset.py
import os
import random
import numpy as np
from glob import glob
from PIL import Image
import torch
import torch.utils.data as data
import torchvision.transforms.functional as F
from torchvision import transforms

# Import your density generator - we will scale the GT density here to fix the magnitude mismatch
from utils.density_utils import generate_density_map_tensor

def random_crop(im_h, im_w, crop_h, crop_w):
    res_h = im_h - crop_h
    res_w = im_w - crop_w
    i = random.randint(0, max(0, res_h))
    j = random.randint(0, max(0, res_w))
    return i, j, crop_h, crop_w

def cal_inner_area(c_left, c_up, c_right, c_down, bbox):
    inner_left = np.maximum(c_left, bbox[:, 0])
    inner_up = np.maximum(c_up, bbox[:, 1])
    inner_right = np.minimum(c_right, bbox[:, 2])
    inner_down = np.minimum(c_down, bbox[:, 3])
    inner_area = np.maximum(inner_right - inner_left, 0.0) * np.maximum(inner_down - inner_up, 0.0)
    return inner_area

class BrokerCrowdDataset(data.Dataset):
    def __init__(self, root_path, crop_size=384, downsample_ratio=8, method='train', enable_gt_density=True):
        self.root_path = root_path
        # Expect filenames like: <name>_RGBT.png (your original pattern). If different, adjust pattern.
        self.rgbt_list = sorted(glob(os.path.join(self.root_path, '*.png')))
        if method not in ['train', 'val', 'test']:
            raise ValueError("method should be 'train'|'val'|'test'")
        self.method = method
        self.c_size = crop_size
        self.d_ratio = downsample_ratio
        self.enable_gt_density = enable_gt_density

        self.rgb_trans = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406],
                                 [0.229, 0.224, 0.225])
        ])
        # If thermal is 1ch, adapt transforms; here we assume thermal stored as 3-channel jpg (as in your repo)
        self.t_trans = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406],
                                 [0.229, 0.224, 0.225])
        ])

    def __len__(self):
        return len(self.rgbt_list)

    def __getitem__(self, index):
        rgbt_path = self.rgbt_list[index]
        # convert filenames from RGBT.png to RGB.jpg, T.jpg, GT.npy (match your data layout)
        rgb_path = rgbt_path.replace('RGBT.png', 'RGB.jpg')
        t_path = rgbt_path.replace('RGBT.png', 'T.jpg')
        gt_path = rgbt_path.replace('RGBT.png', 'GT.npy')

        rgb = Image.open(rgb_path).convert('RGB')
        t = Image.open(t_path).convert('RGB')
        keypoints = np.load(gt_path)  # expected Nx3 array: x,y,nearest_dis

        if self.method == 'train':
            return self.train_transform(rgb, t, keypoints)
        else:
            rgb_t = self.rgb_trans(rgb)
            t_t = self.t_trans(t)
            name = os.path.basename(rgbt_path).split('.')[0]
            return rgb_t, t_t, None, len(keypoints), name

    def train_transform(self, rgb, t, keypoints):
        wd, ht = rgb.size
        st_size = min(wd, ht)
        crop_size = min(self.c_size, st_size)

        i, j, h, w = random_crop(ht, wd, crop_size, crop_size)
        rgb = F.crop(rgb, i, j, h, w)
        t = F.crop(t, i, j, h, w)

        original_kps = keypoints.copy()

        nearest_dis = np.clip(keypoints[:, 2], 4.0, 128.0)
        pts_lu = keypoints[:, :2] - nearest_dis[:, None] / 2.0
        pts_rd = keypoints[:, :2] + nearest_dis[:, None] / 2.0
        bbox = np.concatenate((pts_lu, pts_rd), axis=1)
        inner_area = cal_inner_area(j, i, j + w, i + h, bbox)
        origin_area = nearest_dis * nearest_dis
        ratio = np.clip(inner_area / origin_area, 0.0, 1.0)
        mask = (ratio >= 0.3)

        target = ratio[mask]
        keypoints = keypoints[mask]

        if len(keypoints) == 0:
            keypoints = np.array([[w / 2, h / 2]], dtype=np.float32)
            target = np.array([0.5], dtype=np.float32)
        else:
            keypoints = keypoints[:, :2] - [j, i]

        gt_density = None

        if self.enable_gt_density:
            crop_kps = []
            for p in original_kps:
                x, y = p[0], p[1]
                if j <= x < j + w and i <= y < i + h:
                    crop_kps.append([x - j, y - i])
            if len(crop_kps) > 0:
                # Calculate target size based on downsample ratio
                target_h = h // self.d_ratio
                target_w = w // self.d_ratio
                
                # Scale keypoints to target resolution
                scaled_kps = np.array(crop_kps, dtype=np.float32)
                scaled_kps[:, 0] = scaled_kps[:, 0] * target_w / w
                scaled_kps[:, 1] = scaled_kps[:, 1] * target_h / h
                
                # Adjust sigma based on resolution
                sigma_scale = target_h / h
                adjusted_sigma = max(0.5, 3.0 * sigma_scale)
                
                gt_density = generate_density_map_tensor(
                    scaled_kps, 
                    (target_h, target_w),
                    sigma=adjusted_sigma
                ).unsqueeze(0)
                
                # Verify normalization (remove debug print after confirming it works)
                expected_count = len(crop_kps)
                actual_sum = gt_density.sum().item()
                # Remove the n_batches check - just print always or remove entirely
                # print(f"‚úì GT: count={expected_count}, sum={actual_sum:.2f}, shape={gt_density.shape}")
                
            else:
                target_h = h // self.d_ratio
                target_w = w // self.d_ratio
                gt_density = torch.zeros(1, target_h, target_w, dtype=torch.float32)


        # Random flip
        if random.random() > 0.5:
            rgb = F.hflip(rgb)
            t = F.hflip(t)
            keypoints[:, 0] = w - keypoints[:, 0]
            if gt_density is not None:
                gt_density = torch.flip(gt_density, dims=[2])

        rgb = self.rgb_trans(rgb)
        t = self.t_trans(t)

        if self.enable_gt_density and gt_density is not None:
            # return: rgb, thermal, keypoints_tensor, targets_tensor, st_size, gt_density_tensor
            return rgb, t, torch.from_numpy(keypoints).float(), torch.from_numpy(target).float(), st_size, gt_density
        else:
            return rgb, t, torch.from_numpy(keypoints).float(), torch.from_numpy(target).float(), st_size

# -----------------------------------------------------------------------------
# Collate function
# -----------------------------------------------------------------------------
def crowd_collate(batch):
    """collate supporting optional gt density"""
    # training tuple with gt_density: (rgb,t,kps,targets,st_size,gt_density)
    if len(batch[0]) == 6:
        rgbs, ts, kps_list, targets_list, st_sizes, densities = [], [], [], [], [], []
        for item in batch:
            rgbs.append(item[0])
            ts.append(item[1])
            kps_list.append(item[2])
            targets_list.append(item[3])
            st_sizes.append(item[4])
            densities.append(item[5])
        return torch.stack(rgbs, 0), torch.stack(ts, 0), kps_list, targets_list, torch.tensor(st_sizes, dtype=torch.float32), torch.stack(densities, 0)

    # validation/test: (rgb,t,None,count,name)
    if len(batch[0]) == 5 and (batch[0][2] is None):
        rgb, t, _, count, name = batch[0]
        return rgb.unsqueeze(0), t.unsqueeze(0), None, torch.tensor([count], dtype=torch.float32), name

    # training without density
    rgbs, ts, kps_list, targets_list, st_sizes = [], [], [], [], []
    for item in batch:
        rgbs.append(item[0])
        ts.append(item[1])
        kps_list.append(item[2])
        targets_list.append(item[3])
        st_sizes.append(item[4])
    return torch.stack(rgbs, 0), torch.stack(ts, 0), kps_list, targets_list, torch.tensor(st_sizes, dtype=torch.float32)






--------------------------------------------------------------under models folder


backbone.py

import torch
import torch.nn as nn
import torchvision.models as models

def make_vgg_backbone(pretrained=True):
    """Create VGG16 backbone for feature extraction"""
    backbone = models.vgg16(pretrained=pretrained).features
    # Remove last maxpool to maintain spatial resolution for dense prediction
    backbone = nn.Sequential(*list(backbone.children())[:-1])
    return backbone

def make_resnet_backbone(pretrained=True):
    """Create ResNet50 backbone for feature extraction"""
    resnet = models.resnet50(pretrained=pretrained)
    # Remove final avgpool and fc layers
    backbone = nn.Sequential(*list(resnet.children())[:-2])
    return backbone

class FeatureAdapter(nn.Module):
    """Adapter to match feature dimensions across different backbones"""
    
    def __init__(self, in_channels, out_channels=512):
        super(FeatureAdapter, self).__init__()
        self.adapter = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True)
        )
    
    def forward(self, x):
        return self.adapter(x)
    
    
class InputStemRGB(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv = nn.Conv2d(3, 64, kernel_size=3, padding=1)
    def forward(self, x):
        return self.conv(x)

class InputStemThermal(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv = nn.Conv2d(3, 64, kernel_size=3, padding=1)
    def forward(self, x):
        return self.conv(x)

    
def make_shared_vgg_backbone():
    """
    Creates a partially shared VGG-16 backbone:
    - First layer separate for RGB (3ch) and Thermal (1ch)
    - Rest of VGG shared
    """
    import torchvision.models as models
    vgg = models.vgg16_bn(pretrained=True)

    # Extract all layers except first conv
    shared_layers = vgg.features[1:]   # skip first conv

    return shared_layers



import torch
import torch.nn as nn
import torch.nn.functional as F
from .saai_aligner import SemanticAdversarialAligner
#from backbones import make_vgg_backbone, make_resnet_backbone, FeatureAdapter

# ---------------------------
# SHARED BACKBONE (IMPORTANT)
# ---------------------------
from .backbones import make_shared_vgg_backbone, InputStemRGB, InputStemThermal, FeatureAdapter






class SAAICrowdCounter(nn.Module):
    """Complete SAAI-enhanced crowd counting model with semantic alignment"""
    
    def __init__(self, backbone_name='vgg16', pretrained=True, feature_dim=512):
        super(SAAICrowdCounter, self).__init__()
    #--------------------------------------------------------------------------------------------------
        # Separate first-layer stems
        self.rgb_stem = InputStemRGB()
        self.thermal_stem = InputStemThermal()

        # Shared VGG trunk
        self.shared_backbone = make_shared_vgg_backbone()

        backbone_channels = 512   # since VGG16 outputs 512 channels
    #-------------------------------------------------------------------------------------------------- 
        # Feature adaptation to standard dimension
        if backbone_channels != feature_dim:
            self.rgb_adapter = FeatureAdapter(backbone_channels, feature_dim)
            self.thermal_adapter = FeatureAdapter(backbone_channels, feature_dim)
        else:
            self.rgb_adapter = nn.Identity()
            self.thermal_adapter = nn.Identity()
        
        # SAAI semantic adversarial alignment module
        self.saai_aligner = SemanticAdversarialAligner(feature_dim=feature_dim)
        
        # Feature fusion module (inspired by broker modality approach)
        self.fusion_module = nn.Sequential(
            nn.Conv2d(feature_dim * 2, feature_dim, 3, padding=1),
            nn.BatchNorm2d(feature_dim),
            nn.ReLU(inplace=True),
            nn.Conv2d(feature_dim, feature_dim, 3, padding=1),
            nn.BatchNorm2d(feature_dim),
            nn.ReLU(inplace=True),
            nn.Dropout2d(0.1)
        )
        
        # Multi-scale regression head for density prediction
        self.regression_head = nn.Sequential(
            nn.Conv2d(feature_dim, 256, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 128, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 64, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 1, 1),
            nn.ReLU(inplace=True)  # Ensure non-negative density values
        )
        
    def forward(self, rgb, thermal):
        """
        Forward pass through SAAI-enhanced crowd counter
        Args:
            rgb: [B, 3, H, W] RGB images
            thermal: [B, 3, H, W] Thermal images
        Returns:
            density_map: [B, 1, H', W'] Predicted density maps
            domain predictions for adversarial training
        """
        # Extract features from dual-stream backbones
        # 1. First conv layer (separate for RGB/Thermal)
        rgb_features = self.rgb_stem(rgb)
        thermal_features = self.thermal_stem(thermal)

        # 2. Shared VGG backbone
        rgb_features = self.shared_backbone(rgb_features)
        thermal_features = self.shared_backbone(thermal_features)

        
        # Adapt feature dimensions if needed
        rgb_features = self.rgb_adapter(rgb_features)
        thermal_features = self.thermal_adapter(thermal_features)
        
        # SAAI semantic adversarial alignment
        rgb_aligned, thermal_aligned, domain_pred_rgb, domain_pred_thermal = \
            self.saai_aligner(rgb_features, thermal_features)
        
        # Multi-modal feature fusion
        fused_features = torch.cat([rgb_aligned, thermal_aligned], dim=1)
        fused_features = self.fusion_module(fused_features)
        
        # Density map prediction
        density_map = self.regression_head(fused_features)
        
        return density_map, domain_pred_rgb, domain_pred_thermal


import torch
import torch.nn as nn
import torch.nn.functional as F


class SemanticAdversarialAligner(nn.Module):
    """SAAI-inspired semantic alignment module for RGB-Thermal feature alignment"""
    
    def __init__(self, feature_dim=512, num_prototypes=64, num_heads=8):
        super(SemanticAdversarialAligner, self).__init__()
        self.feature_dim = feature_dim
        self.num_prototypes = num_prototypes
        
        # Semantic prototypes for crowd counting scenes
        self.prototypes = nn.Parameter(torch.randn(num_prototypes, feature_dim))
        nn.init.kaiming_normal_(self.prototypes, mode='fan_out', nonlinearity='relu')
        
        # Cross-modal attention mechanism for RGB-Thermal alignment
        self.cross_attention_rgb = nn.MultiheadAttention(feature_dim, num_heads, batch_first=True)
        self.cross_attention_thermal = nn.MultiheadAttention(feature_dim, num_heads, batch_first=True)
        
        # Feature transformation networks
        self.rgb_transform = nn.Sequential(
            nn.Conv2d(feature_dim, feature_dim, 3, padding=1),
            nn.BatchNorm2d(feature_dim),
            nn.ReLU(inplace=True),
            nn.Conv2d(feature_dim, feature_dim, 1),
            nn.BatchNorm2d(feature_dim)
        )
        
        self.thermal_transform = nn.Sequential(
            nn.Conv2d(feature_dim, feature_dim, 3, padding=1),
            nn.BatchNorm2d(feature_dim),
            nn.ReLU(inplace=True),
            nn.Conv2d(feature_dim, feature_dim, 1),
            nn.BatchNorm2d(feature_dim)
        )
        
        # Domain discriminator for adversarial alignment
        self.domain_classifier = nn.Sequential(
            nn.Linear(feature_dim, 256),
            nn.ReLU(inplace=True),
            nn.Dropout(0.3),
            nn.Linear(256, 128),
            nn.ReLU(inplace=True),
            nn.Dropout(0.3),
            nn.Linear(128, 2)  # RGB vs Thermal
        )
        
        # Semantic consistency projector
        self.semantic_projector = nn.Sequential(
            nn.Linear(feature_dim, 256),
            nn.ReLU(inplace=True),
            nn.Linear(256, feature_dim)
        )

    def prototype_alignment(self, features):
        """Align features with learned semantic prototypes"""
        B, C, H, W = features.shape
        features_flat = features.view(B, C, -1).permute(0, 2, 1)  # [B, HW, C]
        
        # Compute prototype similarities
        prototypes = F.normalize(self.prototypes, dim=1)
        features_norm = F.normalize(features_flat, dim=2)
        similarities = torch.matmul(features_norm, prototypes.T)  # [B, HW, num_prototypes]
        
        # Soft assignment to prototypes
        assignments = F.softmax(similarities, dim=2)
        aligned_features = torch.matmul(assignments, prototypes)  # [B, HW, C]
        
        return aligned_features.permute(0, 2, 1).view(B, C, H, W)

    def forward(self, rgb_features, thermal_features):
        """
        Forward pass for semantic adversarial alignment
        Args:
            rgb_features: [B, C, H, W] RGB feature maps
            thermal_features: [B, C, H, W] Thermal feature maps
        Returns:
            Aligned features and domain predictions for adversarial training
        """
        B, C, H, W = rgb_features.shape
        
        # Transform features through modality-specific networks
        rgb_transformed = self.rgb_transform(rgb_features)
        thermal_transformed = self.thermal_transform(thermal_features)
        
        # Prototype-based semantic alignment
        rgb_proto_aligned = self.prototype_alignment(rgb_transformed)
        thermal_proto_aligned = self.prototype_alignment(thermal_transformed)
        
        # Cross-modal attention alignment
        rgb_flat = rgb_proto_aligned.view(B, C, -1).permute(0, 2, 1)  # [B, HW, C]
        thermal_flat = thermal_proto_aligned.view(B, C, -1).permute(0, 2, 1)
        
        # RGB attended by thermal features
        rgb_attended, _ = self.cross_attention_rgb(rgb_flat, thermal_flat, thermal_flat)
        # Thermal attended by RGB features
        thermal_attended, _ = self.cross_attention_thermal(thermal_flat, rgb_flat, rgb_flat)
        
        # Reshape back to spatial format
        rgb_aligned = rgb_attended.permute(0, 2, 1).view(B, C, H, W)
        thermal_aligned = thermal_attended.permute(0, 2, 1).view(B, C, H, W)
        
        # Add residual connections for stable training
        rgb_final = rgb_aligned + rgb_features
        thermal_final = thermal_aligned + thermal_features
        
        # Domain classification for adversarial training
        rgb_pooled = F.adaptive_avg_pool2d(rgb_final, 1).view(B, -1)
        thermal_pooled = F.adaptive_avg_pool2d(thermal_final, 1).view(B, -1)
        
        domain_pred_rgb = self.domain_classifier(rgb_pooled)
        domain_pred_thermal = self.domain_classifier(thermal_pooled)
        
        return rgb_final, thermal_final, domain_pred_rgb, domain_pred_thermal



------------------------------------------------------------------under utils folder

density_utils.py

import torch
import numpy as np

def generate_density_map_tensor(keypoints, image_shape, sigma=3.0):
    """
    Generate density map tensor from keypoints
    Args:
        keypoints: Tensor [N, 2] with (x, y) coordinates
        image_shape: (H, W) target shape
        sigma: Gaussian sigma for density map
    Returns:
        density_map: Tensor [H, W]
    """
    H, W = image_shape
    density_map = torch.zeros((H, W), dtype=torch.float32)

    if keypoints is None or len(keypoints) == 0:
        return density_map

    if isinstance(keypoints, torch.Tensor):
        pts = keypoints.detach().cpu().numpy()
    else:
        pts = keypoints

    # Precompute Gaussian kernel
    size = int(max(3, 6 * sigma))
    if size % 2 == 0:
        size += 1
        
    ax = torch.arange(-(size // 2), (size // 2) + 1, dtype=torch.float32)
    xx, yy = torch.meshgrid(ax, ax, indexing='ij')
    kernel = torch.exp(-(xx**2 + yy**2) / (2.0 * sigma**2))

    for p in pts:
        x, y = int(p[0]), int(p[1])
        if not (0 <= x < W and 0 <= y < H):
            continue

        # Apply kernel with boundary checking
        y1 = max(0, y - size // 2)
        y2 = min(H, y + size // 2 + 1)
        x1 = max(0, x - size // 2)
        x2 = min(W, x + size // 2 + 1)

        ky1 = max(0, (size // 2) - y)
        ky2 = ky1 + (y2 - y1)
        kx1 = max(0, (size // 2) - x)
        kx2 = kx1 + (x2 - x1)

        density_map[y1:y2, x1:x2] += kernel[ky1:ky2, kx1:kx2]

    person_count = len(pts)
    total_sum = density_map.sum()
    if total_sum > 0 and person_count > 0:
        density_map *= person_count / total_sum

    #print(f"VARUN TEST : Density sum: {density_map.sum():.2f}, Person count: {len(pts)}")


    return density_map

def generate_density_batch(keypoints_list, target_hw, sigma=3.0):
    """
    Generate batch of density maps
    Args:
        keypoints_list: List of keypoint tensors [Ni, 2]
        target_hw: (H, W) target shape
        sigma: Gaussian sigma
    Returns:
        density_batch: Tensor [B, 1, H, W]
    """
    H, W = target_hw
    B = len(keypoints_list)
    out = torch.zeros((B, 1, H, W), dtype=torch.float32)
    
    for i, kps in enumerate(keypoints_list):
        dm = generate_density_map_tensor(kps, (H, W), sigma=sigma)
        out[i, 0] = dm
    
    return out


losses.pyimport torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np


# -----------------------------------------------------
# SSIM structural similarity
# -----------------------------------------------------
def _gaussian_window(channels, window_size=11, sigma=0.2, device="cpu"):
    coords = torch.arange(window_size, dtype=torch.float32, device=device) - window_size // 2
    g = torch.exp(-(coords**2) / (2 * sigma * sigma))
    g = (g / g.sum()).unsqueeze(0)
    w2d = (g.t() @ g).unsqueeze(0).unsqueeze(0)
    return w2d.repeat(channels, 1, 1, 1)


def ssim_loss(pred, target, window_size=11, sigma=0.2, C1=0.01**2, C2=0.03**2):
    B, C, H, W = pred.shape
    device = pred.device
    win = _gaussian_window(C, window_size, sigma, device=device)

    mu_x = F.conv2d(pred, win, padding=window_size//2, groups=C)
    mu_y = F.conv2d(target, win, padding=window_size//2, groups=C)
    mu_x2, mu_y2 = mu_x.pow(2), mu_y.pow(2)
    mu_xy = mu_x * mu_y

    sigma_x = F.conv2d(pred*pred, win, padding=window_size//2, groups=C) - mu_x2
    sigma_y = F.conv2d(target*target, win, padding=window_size//2, groups=C) - mu_y2
    sigma_xy = F.conv2d(pred*target, win, padding=window_size//2, groups=C) - mu_xy

    ssim_map = ((2*mu_xy + C1)*(2*sigma_xy + C2)) / (
        (mu_x2 + mu_y2 + C1)*(sigma_x + sigma_y + C2) + 1e-12
    )
    return 1.0 - ssim_map.mean()


# -----------------------------------------------------
# Regional Count Loss (GAME-style)
# -----------------------------------------------------
def regional_count_loss(pred, target, levels=(1, 2, 3)):
    B, _, H, W = pred.shape
    loss = 0.0

    for L in levels:
        cells = 2 ** L
        h_step, w_step = H // cells, W // cells
        for i in range(cells):
            for j in range(cells):
                ph = slice(i*h_step, (i+1)*h_step if i < cells-1 else H)
                pw = slice(j*w_step, (j+1)*w_step if j < cells-1 else W)
                pred_c = pred[:, :, ph, pw].sum(dim=(2, 3))
                gt_c = target[:, :, ph, pw].sum(dim=(2, 3))
                loss += (pred_c - gt_c).abs().mean()

    return loss / len(levels)


# -----------------------------------------------------
# Main Loss Class
# -----------------------------------------------------
class SAAIBrokerLoss(nn.Module):
    """Hybrid Enhanced Loss = Bayesian + Count + Domain + SSIM + Regional"""

    def __init__(self, alpha=0.1, beta=0.05, gamma=1.0, 
                 w_ssim=0.5, w_regional=0.1, regional_levels=(1, 2, 3)):
        super(SAAIBrokerLoss, self).__init__()
        self.alpha = alpha
        self.beta = beta
        self.gamma = gamma
        self.w_ssim = w_ssim
        self.w_regional = w_regional
        self.regional_levels = regional_levels

        self.mse_loss = nn.MSELoss()
        self.l1_loss = nn.L1Loss()
        self.ce_loss = nn.CrossEntropyLoss()

    def forward(self, density_map, keypoints_list, targets_list,
                domain_pred_rgb, domain_pred_thermal, gt_density_maps=None):
        device = density_map.device
        B = density_map.size(0)

        bayesian_loss, count_loss = torch.tensor(0., device=device), torch.tensor(0., device=device)

        for i in range(B):
            if len(keypoints_list[i]) > 0:
                keypoints = keypoints_list[i].to(device)
                targets = targets_list[i].to(device)

                h_scale = density_map.size(2) / max(keypoints[:, 1].max().item() + 1, density_map.size(2))
                w_scale = density_map.size(3) / max(keypoints[:, 0].max().item() + 1, density_map.size(3))

                x_coords = torch.clamp((keypoints[:, 0] * w_scale).long(), 0, density_map.size(3) - 1)
                y_coords = torch.clamp((keypoints[:, 1] * h_scale).long(), 0, density_map.size(2) - 1)

                predicted_densities = density_map[i, 0, y_coords, x_coords]
                bayesian_loss += self.mse_loss(predicted_densities, targets)

                pred_count = density_map[i].sum()
                gt_count = len(keypoints_list[i])
                count_loss += self.l1_loss(pred_count, torch.tensor(gt_count, dtype=torch.float32, device=device))

        bayesian_loss /= B
        count_loss /= B

        density_mse = torch.tensor(0., device=device)
        ssim_struct = torch.tensor(0., device=device)
        regional = torch.tensor(0., device=device)

        if gt_density_maps is not None:
            if gt_density_maps.shape != density_map.shape:
                gt_density_maps = F.interpolate(gt_density_maps, size=density_map.shape[2:], mode='bilinear', align_corners=False)

            density_mse = self.mse_loss(density_map, gt_density_maps)
            ssim_struct = ssim_loss(density_map, gt_density_maps)
            regional = regional_count_loss(density_map, gt_density_maps, self.regional_levels)

        rgb_labels = torch.zeros(B, dtype=torch.long, device=device)
        t_labels = torch.ones(B, dtype=torch.long, device=device)
        domain_loss = (self.ce_loss(domain_pred_rgb, rgb_labels) + self.ce_loss(domain_pred_thermal, t_labels)) / 2

        if gt_density_maps is not None:
            main_loss = self.gamma * density_mse
        else:
            main_loss = self.gamma * bayesian_loss

        total_loss = main_loss + self.beta * count_loss + self.alpha * domain_loss \
                     + self.w_ssim * ssim_struct + self.w_regional * regional

        return total_loss, {
            "density_loss": main_loss.item(),
            "count_loss": count_loss.item(),
            "domain_loss": domain_loss.item(),
            "ssim_loss": ssim_struct.item(),
            "regional_loss": regional.item(),
            "total_loss": total_loss.item(),
        }



metrics.py

import torch
import numpy as np

def calculate_mae(pred_density, gt_count):
    """Calculate Mean Absolute Error"""
    pred_count = pred_density.sum(dim=(2, 3)).squeeze()
    if isinstance(gt_count, list):
        gt_count = torch.tensor(gt_count, dtype=torch.float32).to(pred_density.device)
    return torch.mean(torch.abs(pred_count - gt_count)).item()

def calculate_mse(pred_density, gt_count):
    """Calculate Mean Squared Error"""
    pred_count = pred_density.sum(dim=(2, 3)).squeeze()
    if isinstance(gt_count, list):
        gt_count = torch.tensor(gt_count, dtype=torch.float32).to(pred_density.device)
    return torch.mean((pred_count - gt_count) ** 2).item()

def calculate_rmse(pred_density, gt_count):
    """Calculate Root Mean Squared Error"""
    return np.sqrt(calculate_mse(pred_density, gt_count))


under Varunpipeline top-folder

dataset- contain broker_crowd_dataset.py
models
utils
RGBT-CC-And-Initial-Broker-contain data
config.py 
import argparse

def get_config():
    parser = argparse.ArgumentParser(description='SAAI Enhanced Crowd Counting')
    
    # Dataset settings
    parser.add_argument('--data_path', type=str, required=True,
                        help='Path to RGBT-CC dataset')
    parser.add_argument('--crop_size', type=int, default=384,
                        help='Random crop size for training')
    parser.add_argument('--downsample_ratio', type=int, default=8,
                        help='Downsample ratio for density maps')
    
    # Model settings
    parser.add_argument('--backbone', type=str, default='vgg16',
                        choices=['vgg16', 'resnet50'],
                        help='Backbone architecture')
    parser.add_argument('--feature_dim', type=int, default=512,
                        help='Feature dimension for SAAI alignment')
    parser.add_argument('--num_prototypes', type=int, default=64,
                        help='Number of semantic prototypes')
    
    # Training settings
    parser.add_argument('--batch_size', type=int, default=8,
                        help='Batch size for training')
    parser.add_argument('--lr', type=float, default=1e-4,
                        help='Learning rate')
    parser.add_argument('--weight_decay', type=float, default=1e-4,
                        help='Weight decay')
    parser.add_argument('--epochs', type=int, default=200,
                        help='Number of training epochs')
    
    # Loss weights
    parser.add_argument('--alpha', type=float, default=0.1,
                        help='Domain adversarial loss weight')
    parser.add_argument('--beta', type=float, default=0.05,
                        help='Semantic alignment loss weight')
    
    # Logging and saving
    parser.add_argument('--save_dir', type=str, default='./checkpoints',
                        help='Directory to save checkpoints')
    parser.add_argument('--log_interval', type=int, default=50,
                        help='Logging interval')
    
    return parser.parse_args()


test.py

import os
import torch
from torch.utils.data import DataLoader
from datasets.saai_crowd_dataset import SAAICrowdDataset, saai_crowd_collate
from models.crowd_counter import SAAICrowdCounter
from utils.metrics import calculate_mae, calculate_rmse
from config import get_config

def test_saai_model():
    """Test function for SAAI crowd counter"""
    config = get_config()
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # Create test dataset
    test_dataset = SAAICrowdDataset(
        root_path=os.path.join(config.data_path, 'test'),
        crop_size=config.crop_size,
        downsample_ratio=config.downsample_ratio,
        method='test',
        enable_domain_labels=False,
        enable_density_maps=True
    )
    
    test_loader = DataLoader(
        test_dataset,
        batch_size=1,
        shuffle=False,
        collate_fn=saai_crowd_collate,
        num_workers=4
    )
    
    # Load model
    model = SAAICrowdCounter(
        backbone_name=config.backbone,
        pretrained=False,
        feature_dim=config.feature_dim
    ).to(device)
    
    checkpoint = torch.load(os.path.join(config.save_dir, 'best_saai_model.pth'))
    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()
    
    total_mae = 0
    total_rmse = 0
    results = []
    
    print("Starting testing...")
    with torch.no_grad():
        for batch_idx, batch_data in enumerate(test_loader):
            rgb, thermal, target, gt_count, name = batch_data[:5]
            
            rgb, thermal = rgb.to(device), thermal.to(device)
            
            pred_density, _, _ = model(rgb, thermal)
            pred_count = pred_density.sum().item()
            
            mae = abs(pred_count - gt_count)
            rmse = (pred_count - gt_count) ** 2
            
            total_mae += mae
            total_rmse += rmse
            
            results.append({
                'name': name[0],
                'gt_count': gt_count,
                'pred_count': pred_count,
                'mae': mae
            })
            
            if batch_idx % 100 == 0:
                print(f'Processed {batch_idx}/{len(test_loader)} images')
    
    final_mae = total_mae / len(test_loader)
    final_rmse = (total_rmse / len(test_loader)) ** 0.5
    
    print(f'Test Results:')
    print(f'MAE: {final_mae:.2f}')
    print(f'RMSE: {final_rmse:.2f}')
    
    return results

if __name__ == '__main__':
    test_saai_model()


shared_backbone_train.py
# train_shared_backbone.py
import os
import argparse
import time
import logging
import json
from datetime import datetime
from tqdm import tqdm

import torch
from torch.utils.data import DataLoader

from datasets.broker_crowd_dataset import BrokerCrowdDataset, crowd_collate
from models.crowd_counter import SAAICrowdCounter
from utils.losses import SAAIBrokerLoss
from utils.metrics import calculate_mae, calculate_rmse

import numpy as np

# -----------------------
# Logging helper
# -----------------------
def setup_logging(save_dir):
    os.makedirs(save_dir, exist_ok=True)
    log_filename = os.path.join(save_dir, f"training_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log")
    fmt = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
    logger = logging.getLogger('train')
    logger.setLevel(logging.INFO)

    fh = logging.FileHandler(log_filename)
    fh.setFormatter(fmt)
    logger.addHandler(fh)

    ch = logging.StreamHandler()
    ch.setFormatter(fmt)
    logger.addHandler(ch)

    return logger, log_filename

# -----------------------
# Broker GAME calculation (single-sample)
# -----------------------
def calculate_broker_game_single(pred_map, gt_count):
    """pred_map numpy 2D array, gt_count integer"""
    h, w = pred_map.shape
    results = {}
    # GAME(0)
    results[0] = abs(pred_map.sum() - float(gt_count))
    # GAME(1..3)
    for level in [1, 2, 3]:
        S = 2 ** level
        total = 0.0
        for i in range(S):
            for j in range(S):
                h0 = i * h // S
                h1 = (i + 1) * h // S
                w0 = j * w // S
                w1 = (j + 1) * w // S
                region_pred = pred_map[h0:h1, w0:w1].sum()
                region_area = (h1 - h0) * (w1 - w0)
                region_gt = float(gt_count) * (region_area / (h * w))
                total += abs(region_pred - region_gt)
        results[level] = total
    return results

# -----------------------
# Validation
# -----------------------
def validate(model, val_loader, device, logger):
    model.eval()
    game_sums = {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0}
    mse_acc = 0.0
    n = 0
    with torch.no_grad():
        for batch in val_loader:
            # val collate returns: rgb, t, None, count, name
            rgb, thermal, _, gt_count, name = batch
            rgb = rgb.to(device)
            thermal = thermal.to(device)

            pred_density, _, _ = model(rgb, thermal)   # [1,1,H,W]

            if n == 0:
                print(f"\nüîç Validation Debug:")
                print(f"  Input RGB shape: {rgb.shape}")
                print(f"  Pred density shape: {pred_density.shape}")
                print(f"  Pred density sum: {pred_density.sum().item():.2f}")
                print(f"  Pred density min/max: {pred_density.min().item():.4f} / {pred_density.max().item():.4f}")
                print(f"  GT count: {gt_c}")
                
            
            pred_map = pred_density[0, 0].cpu().numpy()
            if isinstance(gt_count, torch.Tensor):
                gt_c = int(gt_count.item())
            elif isinstance(gt_count, (list, tuple, np.ndarray)):
                gt_c = int(gt_count[0])
            else:
                gt_c = int(gt_count)

            games = calculate_broker_game_single(pred_map, gt_c)
            for k in game_sums:
                game_sums[k] += games[k]

            pred_cnt = pred_map.sum()
            mse_acc += (pred_cnt - gt_c) ** 2
            n += 1

    # averages
    for k in game_sums:
        game_sums[k] /= float(max(n, 1))
    rmse = float(np.sqrt(mse_acc / max(n, 1)))
    logger.info(f"VAL  GAME0:{game_sums[0]:.2f}  GAME1:{game_sums[1]:.2f}  GAME2:{game_sums[2]:.2f}  GAME3:{game_sums[3]:.2f}  RMSE:{rmse:.2f}")
    return game_sums, rmse

# -----------------------
# Main training script
# -----------------------
def get_args():
    p = argparse.ArgumentParser()
    p.add_argument('--data-path', required=True)
    p.add_argument('--save-dir', default='./Results_shared')
    p.add_argument('--epochs', type=int, default=50)
    p.add_argument('--batch-size', type=int, default=8)
    p.add_argument('--lr', type=float, default=1e-4)
    p.add_argument('--crop-size', type=int, default=384)
    p.add_argument('--downsample-ratio', type=int, default=8)
    p.add_argument('--workers', type=int, default=4)
    p.add_argument('--enable-enhanced-loss', action='store_true', dest='enable_enhanced_loss')
    return p.parse_args()

def main():
    args = get_args()
    logger, log_file = setup_logging(args.save_dir)
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    logger.info(f"üöÄ Using device: {device}")
    logger.info(f"üìù Training log: {log_file}")

    # dataset paths
    train_root = os.path.join(args.data_path, 'train')
    val_root = os.path.join(args.data_path, 'val')

    logger.info("üìÇ Creating datasets...")
    train_set = BrokerCrowdDataset(train_root, crop_size=args.crop_size,
                                  downsample_ratio=args.downsample_ratio,
                                  method='train', enable_gt_density=args.enable_enhanced_loss)
    val_set = BrokerCrowdDataset(val_root, crop_size=args.crop_size,
                                downsample_ratio=args.downsample_ratio,
                                method='val', enable_gt_density=False)

    logger.info(f"   Train: {len(train_set)}  Val: {len(val_set)}  GT density during train: {'yes' if args.enable_enhanced_loss else 'no'}")

    train_loader = DataLoader(train_set, batch_size=args.batch_size, shuffle=True,
                              num_workers=args.workers, collate_fn=crowd_collate, pin_memory=True, drop_last=True)
    val_loader = DataLoader(val_set, batch_size=1, shuffle=False,
                            num_workers=max(0, args.workers//2), collate_fn=crowd_collate)

    # model
    model = SAAICrowdCounter(backbone_name='vgg16', pretrained=True, feature_dim=512).to(device)

    # if using shared backbone variant, some code sets rgb/thermal stems + shared trunk inside model
    # Ensure thermal stem accepts 3 channels if dataset uses 3-channel thermal images
    if hasattr(model, 'thermal_stem') and hasattr(model.thermal_stem, 'conv'):
        # replace with 3-channel conv (if your thermal input is single-channel, change to 1)
        model.thermal_stem.conv = torch.nn.Conv2d(3, 64, kernel_size=3, padding=1).to(device)

    # parameter groups: try to detect whether model uses shared_backbone stems or separate backbones
    backbone_params = []
    if hasattr(model, 'shared_backbone'):
        backbone_params += list(model.rgb_stem.parameters()) if hasattr(model, 'rgb_stem') else []
        backbone_params += list(model.thermal_stem.parameters()) if hasattr(model, 'thermal_stem') else []
        backbone_params += list(model.shared_backbone.parameters())
    else:
        # backwards compatibility with two-backbone model style
        backbone_params += list(getattr(model, 'rgb_backbone').parameters()) if hasattr(model, 'rgb_backbone') else []
        backbone_params += list(getattr(model, 'thermal_backbone').parameters()) if hasattr(model, 'thermal_backbone') else []

    saai_params = list(model.saai_aligner.parameters()) if hasattr(model, 'saai_aligner') else []
    head_params = list(model.fusion_module.parameters()) + list(model.regression_head.parameters())

    optimizer = torch.optim.Adam([
        {'params': backbone_params, 'lr': args.lr * 0.1},
        {'params': saai_params, 'lr': args.lr},
        {'params': head_params, 'lr': args.lr},
    ], weight_decay=1e-4)

    # criterion: note your SAAIBrokerLoss must accept gt_density_maps or None
    criterion = SAAIBrokerLoss()

    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.7)

    # tracking
    best_game0 = float('inf')

    logger.info("Start training...")
    for epoch in range(1, args.epochs + 1):
        t0 = time.time()
        model.train()
        running_loss = 0.0
        n_batches = 0

        pbar = tqdm(train_loader, desc=f"Epoch {epoch}/{args.epochs}", ncols=120)
        for batch in pbar:
            # batch shapes:
            # training: rgb, thermal, keypoints_list, targets_list, st_size, (optional) gt_density
            if len(batch) == 6:
                rgb, thermal, kps_list, targets_list, st_sizes, gt_density = batch
                gt_density = gt_density.to(device)
            else:
                rgb, thermal, kps_list, targets_list, st_sizes = batch
                gt_density = None

            rgb = rgb.to(device)
            thermal = thermal.to(device)

            optimizer.zero_grad()
            pred_density, domain_pred_rgb, domain_pred_thermal = model(rgb, thermal)

            # if n_batches == 0:  # First batch only
            #     print(f"\nüîç Shape Debug:")
            #     print(f"  Input RGB: {rgb.shape}")
            #     print(f"  Pred density: {pred_density.shape}")
            #     if gt_density is not None:
            #         print(f"  GT density: {gt_density.shape}")
            #         print(f"  GT density sum (first sample): {gt_density[0].sum().item():.2f}")
            #         print(f"  Pred density sum (first sample): {pred_density[0].sum().item():.2f}")
            #     print(f"  Downsample ratio: {args.downsample_ratio}")
            total_loss, loss_dict = criterion(pred_density, kps_list, targets_list,
                                              domain_pred_rgb, domain_pred_thermal,
                                              gt_density_maps=gt_density)
            total_loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()

            running_loss += total_loss.item()
            n_batches += 1

            pbar.set_postfix({'Loss': f'{running_loss / n_batches:.4f}', 'Den': f"{loss_dict['density_loss']:.4f}",
                              'Dom': f"{loss_dict['domain_loss']:.4f}", 'Count': f"{loss_dict['count_loss']:.4f}"})

        scheduler.step()
        epoch_time = time.time() - t0
        logger.info(f"Epoch {epoch} finished in {epoch_time:.1f}s. Train loss: {running_loss / max(1, n_batches):.6f}")

        # validate every 5 epochs (you asked earlier)
        if epoch % 5 == 0 or epoch == 1:
            game_results, rmse = validate(model, val_loader, device, logger)
            game0 = game_results[0]
            if game0 < best_game0:
                best_game0 = game0
                ckpt_path = os.path.join(args.save_dir, f'best_game0_epoch{epoch}.pth')
                torch.save({
                    'epoch': epoch,
                    'model_state_dict': model.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'game0': game0,
                }, ckpt_path)
                logger.info(f"‚úî Saved best checkpoint to {ckpt_path}")

    logger.info("Training completed.")

if __name__ == "__main__":
    main()
